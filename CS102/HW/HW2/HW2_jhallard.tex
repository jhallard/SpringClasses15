
% Programming/Coding Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
% This template uses a Perl script as an example snippet of code, most other
% languages are also usable. Configure them in the "CODE INCLUSION 
% CONFIGURATION" section.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle} % Top center head
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule
\newcommand{\tab}{\hspace*{3em}}

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%   CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Perl} % Load Perl syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Perl, % Use Perl in this example
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
        keywordstyle=[2]\color{Purple}, % Perl function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers                                         
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Perl functions not included in the default language here
        morekeywords={rand},
        %
        % Put Perl function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
        %
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\perlscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[caption=#2,label=#1]{#1.pl}
\end{itemize}
}

%----------------------------------------------------------------------------------------
%   DOCUMENT STRUCTURE COMMANDS
%   Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%   NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Homework\ \#2} % Assignment title
\newcommand{\hmwkDueDate}{Tuesday,\ April\ 14,\ 2015} % Due date
\newcommand{\hmwkClass}{CMPS\ 102} % Course/class
\newcommand{\hmwkClassTime}{4:00pm} % Class/lecture time
\newcommand{\hmwkClassInstructor}{Warmuth} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{John Allard \ 1437547
} % Your name


%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%   USER SETTINGS
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}



%----------------------------------------------------------------------------------------
%   TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%   TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

% \tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%   PROBLEM 1
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}
 Design 3 algorithms based on binary min heaps that find the $k$th smallest \# out of a set of $n$ \#'s in time:
\begin{enumerate}
\item[a)] $O(n \log k)$
\item[b)] $O(n + k \log n)$
\item[c)] $O(n + k \log k)$
\end{enumerate}

Use the heap operations (here $s$ is the size):
\begin{itemize}
\item   Insert, delete: $O(\log s)$
\item   Buildheap: $O(s)$
\item   Smallest: $O(1)$
\end{itemize}
  
Give high level descriptions of the 3 algorithms and briefly reason correctness and running time. Part c) is the most challenging.


\begin{enumerate}

\item \textbf{Part A} - I found this to be a very wierd problem, any attempts to get the required run-time had me going out of my way to find a slower
than optimal algorithm. I know from looking at the runtime that we need to perform $n$ total insertion or deletion operations on a heap of size $k$, which would give runtime $O(n\log(k))$, but there didn't seem to be a natural way of doing so. Edit - so after seeing the hint by the TA on piazza, I think I know how to build the algorithm. We start by building a max-heap of the first k elements in the array. We then scan through the array, and everytime we find a smaller element we delete the max element from the heap and insert the new smaller element. Since we will continually add the smallest elements to a max-heap of constant size k, at the end the kth smallest (the largest of the k smallest elements) will be at the top of the heap and we can simply grab that value with a single function call. \\

\problemAnswer{
\texttt{// A = array of n elements of arbitray order} \\
\texttt{ 1.} \texttt{ findp(A) } \\
\texttt{ 2.} \texttt{ \tab H = BuildMaxHeap(A[1..k]) // build max-heap of first k elements} \\
\texttt{ 3.} \texttt{ \tab for i in [k..n] // O(n)} \\
\texttt{ 4.} \texttt{ \tab \tab if A[i] < Largest(H) // if new val belongs in heap} \\
\texttt{ 5.} \texttt{ \tab \tab \tab Delete(H) // remove top value} \\
\texttt{ 5.} \texttt{ \tab \tab \tab Insert(H, A[i]) // push new val into heap} \\
\texttt{ 6.} \texttt{ \tab return Largest(H)} \\
}

The algorithm starts by constructing a heap on k elements. This is key, because this lets us achieve the log(k) insetion and deletion time that is needed. We then iterate over the rest of the elements (on the order N, could be much less if k is close to n), performing a single deletion and insertion operation if we find an element that is smaller than the max item in the max heap. Doing this ensures that the smallest values are inserted into the heap, and since we start with k values and only perform single deletions and insertions together, the size of the heap will always remain constant at k. Thus at the end, we will have a max-heap containing the k smallest elements in A. The max of these elements will be at the top of the heap, and it will be the kth smallest element of A (the largest of the k smallest elements), so we return it. This algorithm runs for O(n) iterations, performing an insert and deletion operation each iteration for the worse case. Thus the run-time is : \\
$$ (n-k)*2*\lg(k) = 2*n*\log(k) - 2*k = O(n\log(k))$$


\item \textbf{Part B} - Designing an algorithm to run in $O(n + k\log(n))$ seemed the most inutive to me. My algorithm is like heapsort, except it stops after $k$ iterations, which reduces its run time from $O(n\log(n))$ to $O(n + k\log(n))$. \\[.15in]
\problemAnswer{
\texttt{// A = array of n elements of arbitray order} \\
\texttt{ 1.} \texttt{ findp(A) } \\
\texttt{ 2.} \texttt{ \tab H = BuildHeap(A) // O(n)} \\
\texttt{ 3.} \texttt{ \tab for i in [1..k-1] // O(k)} \\
\texttt{ 4.} \texttt{ \tab \tab Delete(H) // O(log(n))} \\
\texttt{ 5.} \texttt{ \tab return Smallest(H) // O(1)} \\
\texttt{ 6.} \texttt{ \tab // O(n) + O(k)*O(logn) + O(1)} \\
}

The algorithm starts by constructing a heap over all $n$ elements, which is where the $O(n)$ term comes from in the runtime. Now, we simply perform $k-1$ deletion operations, which each take $O(\log(n))$ time to complete. After these operations, the $k$th smallest element will be at the top of the heap, so we can perform a simple Smallest retrieval operation, which will give us the $k$th smallest element. Runtime - $O(n)$ for Buildheap, $O(k)$ iterations of delete-min at a cost of $O(\log(n))$ gives a total run-time of $O(n + k\log(n))$. \\


\item \textbf{Part C} - 

\end{enumerate}

\end{homeworkProblem}


%----------------------------------------------------------------------------------------
%   PROBLEM 2
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}

Consider the following sorting algorithm for an array of numbers (Assume the size $n$ of the array is divisible by 3):
\begin{itemize}
\item   Sort the initial 2/3 of the array.
\item   Sort the final 2/3 and then again the initial 2/3.
\end{itemize}

Reason that this algorithm properly sorts the array. What is its running time?

\textbf{Proof of Correctness} \\
 I am assuming that this is a recursive definition, and that we recurse until we reach two elements at which point we just swap them into place with a single operation. Let $P(n)$ be the statement `for $n \geq 1$, an array $A$ of length $n$ on elements of arbitrary order will be corectly sorted by the 2/3rds sorting algorithm.'

 \textbf{Base Case} \\
 $P(n=1)$ - If $n=1$, there is only one element to be sorted so we just return the array. Confirmed.\\
 $P(n=2)$ - If $n=2$. no recursion is needed, we simply perform a single comparison and swap the items into place, then return the array. Confirmed. \\
 % $P(n=3)$ - If $n=3$, then we need to recurse once. Call the three indices of the array $A_1, A_2,$ and $A_3$. The algorithm will start by recursing on the first two elements, which will perform a single comparison and swap them into place. We now know that whichever element is in $A_2$ is greater or equal to the element in index $A_1$. Next, we recurse on elements $A_2$ and $A_3$, swapping them into place. After this operation, whatever element is in index $A_3$ is greater than what is in $A_2$, and by extension of the result of the last recursive call, the value in $A_3$ is in the correct place (greater or equal to both other elements). Because $A_3$ is correct, we know that $A_1$ and $A_2$ contain the smallest and middle values of the array, but possibly out of order. One final recursive call on $A_1$ and $A_2$ swaps these two final elements into correct order, and the array is now sorted. \\

 \textbf{Inductive Step} \\
 For $n \geq 3$ where n is a multiple of 3, assume that $P(k)$ is true for $3 \leq k \leq n$, i.e. that an array of length $k$ can be sorted correctly by the given sorting algorithm. \\
 Start with an array of size $n+3$ (the next multiple of 3). Let $A_1, A_2,$ and $A_3$ represent the 1st, 2nd, and final thirds of the array indices. 
 On the first call we attempt to recurse on $A_1 \cup A_2$.
 Because this sub-array is of size $\frac{2}{3} (n+3)$, which is less than or equal to $n$ for $n \geq 3$, we can apply the inductive hypothesis on this subarray. \\
 After applying the inductive hypothesis, $A_1 \cup A_2$ will be sorted properly, and thus all of the elements in $A_2$ will be at least as great as the elements in $A_1$.
 $$\forall x \in A_1, y \in A_2, x \leq y \tab (1)$$ 
 We then recurse on $A_2 \cup A_3$, once again the inductive hypothesis applies by the same argument given above, which means $A_2 \cup A_3$ is sorted.
 This implies that all elements in $A_3$ are at least as great as those in $A_2$.
  $$\forall x \in A_2, y \in A_3, x \leq y \tab (2)$$ 
 If you combine this fact with the result of the last recursive call (1), we deduce the following :
   $$\forall x \in A_1 \cup A_2, y \in A_3, x \leq y \tab (3)$$ 
 This means that all elements in $A_3$ are in the right place. The final 3rd of the array being in the right place implies that all of the elements in the indices $A_1 \cup A_2$ belong in that first 2/3rds of the array, but they are possbily scattered and out of orderby our last sorting call. \\
 With one final recursive call on $A_1 \cup A_2$, our inductive hypothesis once again applies and (1) is restored. (3) is still valid because the last sort only rearranged items in $A_1$ and $A_2$, which were all less than or equal to items in $A_3$ to begin with. Combining (1) and (3) : \\
 $$\forall x \in A_1, y \in A_2, z \in A_3, x \leq y \leq z \tab (4)$$
 Add to this the fact that the individual thirds are correctly sorted internally (by our ind. hyp.), and we have shown that the entire array of $n+3$ elements has been properly sorted. /// \\

\textbf{Runtime} \\
This algorithm is slightly odd be it does almost no work while dividing nor when recombining. When we get down to a length less than 3, we simply compare pairs of elements are swap them if necessary. Only at the bottom level do we perform any comparisons, on every other level we simply recurse 3 times on an input of size 2/3rds the current sub-array size. Thus the recurrence is : \\
    \[
    T(n) \left \{
      \begin{tabular}{c l}
      $= O(1)$ & if  $1 \leq n \leq 2$ \\
      $\leq$ 3T(2n/3) + O(1) & \text{ if } $n \geq 3$  \\
      \end{tabular}
    \right \}
    \]

    We can apply the master theorum with $a = 3$, $b=3/2$, and $f(n) = O(1)$. $\log_b(a) = log_{\frac{3}{2}}(3) = 2.7095$. The exponent on $f$ is $0$, so $f = O(n^{\log_b(a)-\epsilon})$, thus we are in case A of the master theorem. This means that :
    $$ T(n) = \Theta(n^{\log_{1.5}(3)} = n^{2.7095}) $$ ///


\end{homeworkProblem}


\begin{homeworkProblem}

KT, problem 1, p 246. :
You are interested in analyzing some hard-to-obtain data from two separate databases. Each database contains n numerical values, so there are 2n values total and you may assume that no two values are the same. You’d like to determine the median of this set of 2n values, which we will define here to be the nth smallest value. \\
However, the only way you can access these values is through queries to the databases. In a single query, you can specify a value k to one of the two databases, and the chosen database will return the kth smallest value that it contains. Since queries are expensive, you would like to compute the median using as few queries as possible. Give an algorithm that finds the median value using at most O(log n) queries \\

Since I am required to write an algorithm that runs in $O(\log n)$ quries, I know that I have to divide my search space by a constant multiple every constant number of queries, like how binary search narrows the search space by a factor of a half every iteration. Like binary search, I'll start by looking in the middle of the data sets, which is the $\frac{n}{2}$ smallest number in each database of size $n$. The key idea that I'll use is that the median of the data set has to have a value that is between the medians of the individual data-sets. This is because if get the $\frac{n}{2}$ smallest number from both data-sets, we know there are at minimum going to be n-numbers smaller than the greater of the two medians. I'll go into this in more detail in the proof of correctness. \\ 

\problemAnswer{
\texttt{// db\_n = data base \#n} \\
\texttt{// db\_query(k, db\_n) returns the kth smallest item in db\_n } \\
\texttt{// pow(x, n) raises x to the nth power } \\
\texttt{ 1.} \texttt{ FindMedian(db\_1, db\_2, n) } \\
\texttt{ 2.} \texttt{ \tab ind\_1 = n/2; // get the median value \#1 } \\
\texttt{ 3.} \texttt{ \tab ind\_2 = n/2; // get the median value \#2} \\
\texttt{ 4.} \texttt{ \tab for k in [2..log(n)] : // log(n) iterations to find median} \\
\texttt{ 4.} \texttt{ \tab \tab med\_1 = db\_query(ind\_1, db\_1) // get ind. median value} \\
\texttt{ 4.} \texttt{ \tab \tab med\_2 = db\_query(ind\_2, db\_2) // get ind. median value} \\
\texttt{ 5.} \texttt{ \tab \tab if med\_2 < med\_1 : } \\
\texttt{ 6.} \texttt{ \tab \tab \tab ind\_1 = ind\_1 - n/pow(2,k) // recurse in the lower half} \\
\texttt{ 7.} \texttt{ \tab \tab \tab ind\_2 = ind\_2 + n/pow(2,k) // recurse in the upper half} \\
\texttt{ 8.} \texttt{ \tab \tab else : // med\_1 > med\_2 (no dups) } \\
\texttt{ 9.} \texttt{ \tab \tab \tab ind\_1 = ind\_1 + n/pow(2,k) } \\
\texttt{ 10.} \texttt{ \tab \tab \tab ind\_2 = ind\_2 - n/pow(2,k) } \\
\texttt{ 11.} \texttt{ \tab // at this point the median is the smaller of the two ind\_n vals} \\
\texttt{ 12.} \texttt{ \tab if med\_1 < med\_2 : } \\
\texttt{ 13.} \texttt{ \tab \tab return med\_1} \\
\texttt{ 14.} \texttt{ \tab else : } \\
\texttt{ 15.} \texttt{ \tab \tab return med\_2} \\
\texttt{ 16.} \texttt{ \tab // 2+ [lg(n)-1] = lg(n)+1 = O(log(n)) db queries} \\
}

Continuing from above, I start by getting the two medians of the respective data-bases, \texttt{ind\_1} and \texttt{ind\_2}. Because these are the
n/2 smallest number in their respective sets, there are at least n numbers smaller than the max of the two medians. We also know that the median has to be greater than or equal to the minimum of the two individual medians, otherwise it would no longer be the nth smallest number in the union of the two data-bases. \\
Since we can pin the median to being between the two individual medians, we can then iterate through half the search space by looking only in the halves of the databases that could possibly contain the global median. If \texttt{ind\_1} $<$ \texttt{ind\_2}, then we know that the median must either be in the lower half of db\_1 or in the upper half of db\_2, so we narrow the search space to those halves by adjusting the index. If \texttt{ind\_1} $>$ \texttt{ind\_2}, we do the opposite and narrow the search space to the upper half of db\_1 and the lower half of db\_2. Since we are narrowing the search space of each data-base of length n by half each time, we only need to perform lg(n) iterations to reduce the halves to single elements. Once we have done this, \texttt{ind\_1} and \texttt{ind\_2} will the nth and n+1st smallest items (in some order) in the total data set. This means we can simply return the smaller of the two values, which will be nth smallest value in the combined data-set, which is exactly what the algiorithm is supposed to do. \\

\textbf{Runtime Analysis} - Given two datasets of unique elements in which we can only query individual sets for the kth smallest element, the algorithm given above will run in time O(log(n)). To start, notice that we always iterate lg(n)-1 times, this is because we are cutting the space in half every time and we start the for-loop already at the middle of the search spaces. Each iteration of the loop we perform 2 query operations, so f(n) = 2. Thus the recurrance for this algorithm is :

$$T(n) = T(n/2) + 2$$

Which, can be solved using the master theorum : 

$$ \log_b(a) = \log_2(1) = 0, f(n) = 2*n^0. \text{ Case \#2} $$

$$ T(n) = O(log(n)) $$
\end{homeworkProblem}



\begin{homeworkProblem}

Suppose you are choosing between the following 3 algorithms:
\begin{enumerate}
\item Algorithm $A$ solves problems by dividing them into 5 subproblems of half the size, recursively solving each subproblem, and then combining the solutions in linear time.
\item Algorithm $B$ solves problems of size n by recursively solving 2 subproblems of size $n-1$ and the combining the solutions in constant time.
\item Algorithm $C$ solves problems of size $n$ by dividing them into nine subproblems of size $n/3$, recursively solving each subproblem, and the combining the solution in $O(n^2)$ time.
\end{enumerate}

What are the running times of each of these algs. (in big-O notation), and which would you choose?

\begin{enumerate}

\item The recurrance (extracted from the desceription) is as follows :
$$ T(n) = 5T(n/2) + O(n) $$

Since $\log_2(5) > 1$, then $f(n) = O(n^{\log_2(5)-\epsilon}$, we are in case one of the master theorum. Thus :
$$ T(n) = O(n^{log_2(5)) = O(n^{2.322})} ) $$

\item The master theorem doesn't directly apply to this one, so I used iteration and substitution like is used in the derivation of the master theorem. The recurrence is : \\
$$ T(n) = 2T(n-1)+c $$
Substituting $T(n-1)$ and multiplying out: 
$$ T(n) = 4T(n-2) + 3c $$
Substituting in $T(n-2)$ and multiplying :
$$ T(n) = 8T(n-3)+ 7c $$

Generalizing from this pattern :
$$ T(n) = 2^kT(n-k) + (k-1)c $$

If we let $k$ go to $n$, which bottoms out the recurrance, we get :
$$ T(n) = 2^nT(0) + (n-1)c $$
$$ T(n) = 2^n + O(n)$$
$$T(n) = O(2^n) $$

Thus the run-time is of exponential order for this algortihm. This makes sense to me, as we double the amount of problems we have to solve for every call while only reducing the amount of work each subsequent call has to by 1.  \\

\item The recurrance (extracted from the desceription) is as follows :
$$ T(n) = 9T(n/3) + O(n^2) $$

Since $\log_3(9) = 2$, then $f(n) = \Theta(n^{log_3(9))} = n^2$, we are in case two of the master theorum. Thus :
$$ T(n) = O(n^2\log(n)) $$

Between the first and the third options, the third option just barely wins out. This is because $n^xlog(n) = O(n^{x+\epsilon})$ for all $x \geq 1$. Letting $x = 2, \epsilon = \log_2(5)-2$ we can see that the third option is Big-O of the first, which makes the third option the better choice. Option \#2 is obviously the worse choice with super-polynomial run-time, so both option \#3 and option \#1 are better choices than it. This leaves option \#3 as the best choice for fastest algorithm.

\end{enumerate}

\end{homeworkProblem}



\begin{homeworkProblem}

 The \textit{Hadamard matrices} $H_0, H_1, H_2, \ldots$ are defined as follows:
\begin{itemize}
\item $H_0$ is the $1 \times 1$ matrix $[1]$
\item For $k>0, H_k$ is the $2^k \times 2^k$ matrix
\end{itemize}
\begin{displaymath}
H_k =
\begin{bmatrix}
H_{k-1} & H_{k-1} \\
H_{k-1} & -H_{k-1} 
\end{bmatrix}
\end{displaymath}
Show that if $v$ is a column vector of length $n = 2^k$, then the matrix-vector product $H_kv$ can be calculated using $O(n \log n)$ operations. Assume that all the numbers involved are small enough that basic arithmetic operations like addition and multiplication take unit time.


\end{homeworkProblem}

\begin{homeworkProblem}

 \textbf{(Extra Credit)} The square of a matrix $A$ is its product with itself, $A A$.

\begin{enumerate}
\item Show that 5 multiplications are sufficient to compute the square of a $2 \times 2$ matrix.
\item What is wrong with the following algorithm for computing the square of an $n \times n$ matrix.\\
``Use a divide-and-conquer approach as in Strassen's algorithm, except that instead of getting 7 subproblems of size $n/2$, we now get 5 subproblems of size $n/2$ thanks to part a). Using the same analysis as in Strassen's algorithm we can conclude that the algorithm runs in time $O(n^{\log_2 5})$.''
\item In fact, squaring matrices is no easier that matrix multiplication. Show that if $n \times n$ matrices can be squared in time $O(n^c)$, then any two $n \times n$ matrices can be multiplied in time $O(n^c)$.
\end{enumerate}

\end{homeworkProblem}

%----------------------------------------------------------------------------------------

\end{document}